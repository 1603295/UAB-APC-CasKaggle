{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cas Kaggle: \n",
    "## Preferències d'una persona en pelicules a partir de la seva personalitat."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom: Gerard Asbert Marcos\n",
    "\n",
    "Niu: 1603295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "dataset0 = pd.read_csv(\"2018-personality-data.csv\")\n",
    "dataset0.drop(columns=dataset0.columns[-26:], axis=1, inplace=True)\n",
    "dataset1 = pd.read_csv(\"2018_ratings.csv\")\n",
    "dataset1.drop(columns=dataset1.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "dataset1.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(dataset1, dataset0, on='userid')\n",
    "dataset.rename(columns={' movie_id': 'movie_id', ' rating': 'rating', ' openness': 'openness', ' agreeableness': 'agreeableness', ' emotional_stability': 'emotional_stability', ' conscientiousness': 'conscentiousness', ' extraversion': 'extraversion', ' assigned metric': 'assigned metric', ' assigned condition': 'assigned condition'}, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara que ja tenim les dades que considerem necesàries en un sol dataframe, el primer que farem serà passar els atributs 'assigned metric' i 'assigned condition' a valors numerics.\n",
    "\n",
    "assigned metric: serendipity, popularity, diversity, all\n",
    "asigned condition: high, medium, low, default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(dataset['assigned metric'])\n",
    "transformed = encoder.transform(dataset['assigned metric'])\n",
    "data0 = pd.DataFrame(transformed)\n",
    "dataset = pd.concat([dataset, data0], axis=1).drop(['assigned metric'], axis=1)\n",
    "dataset.rename(columns={0: 'all', 1: 'diversity', 2: 'popularity', 3: 'serendipity'}, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(dataset['assigned condition'])\n",
    "transformed = encoder.transform(dataset['assigned condition'])\n",
    "data0 = pd.DataFrame(transformed)\n",
    "dataset = pd.concat([dataset, data0], axis=1).drop(['assigned condition'], axis=1)\n",
    "dataset.rename(columns={0: 'default', 1: 'high', 2: 'low', 3: 'medium'}, inplace=True)\n",
    "dataset.drop([\"default\", \"userid\"], axis=1, inplace=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com l'únic que ens interessa de la persona són les dades sobre la seva personalitat, ens hem desfet del userId. També ens hem desfet de l'atribut default ja que és el mateix que l'atribut all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "També, passem l'atribut objectiu 'rating' a ints, per poder-los utilitzar com a classes, i així convertint-ho en un problema de classificació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.replace({'rating': {0.5: 1, 1: 2, 1.5: 3, 2: 4, 2.5: 5, 3: 6, 3.5: 7, 4: 8, 4.5: 9, 5: 10}})\n",
    "dataset = dataset.astype({'rating':'int'})\n",
    "\n",
    "data = dataset.values\n",
    "x = data[:, np.array([True, False, True, True, True, True, True, True, True, True, True, True, True, True])]\n",
    "y = data[:, 1]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().apply(lambda s: s.apply('{0:.2f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps + Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = dataset.corr()\n",
    "plt.figure(figsize=[10, 8])\n",
    "sns.heatmap(np.abs(co), annot=True, linewidths=0.5, annot_kws={\"size\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(figsize=[12, 12], ylabelsize=8, )\n",
    "#rel = sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalització + Crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer escollirem el tipus de normalització que volem utilitzar en el nostre dataset entre aquestes 8 opcions: Sense Normalització, Minmax Scaler, Standard Scaler, Robust Scaler, Yeo-Johnson Scaler (PowerTransformer), Quantile Transformer (Uniform), Quantile Transformer (Gaussian), Normalizer. Per mesurar quina normalització és la millor, els hi aplicarem una simple regressió logistica i veurem quina dona una millor accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com es pot veure les dades están desbalançejades, ja que hi ha aproximadament 11 vegades més instáncies en ratings de 8 que en ratings de 1, i per lo tant utilitzaré un stratified k-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "from statistics import mean\n",
    "\n",
    "x = x.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "minmax = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "robust = sklearn.preprocessing.RobustScaler()\n",
    "\n",
    "yeoJohnson = sklearn.preprocessing.PowerTransformer()\n",
    "\n",
    "quantileUniform = sklearn.preprocessing.QuantileTransformer()\n",
    "\n",
    "quantileGaussian = sklearn.preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "normalizer = sklearn.preprocessing.Normalizer()\n",
    "\n",
    "\n",
    "logReg = sklearn.linear_model.LogisticRegression(max_iter=1000)\n",
    "\n",
    "skf = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True)\n",
    "accuracy_sense = []\n",
    "accuracy_minmax = []\n",
    "accuracy_standard = []\n",
    "accuracy_robust = []\n",
    "accuracy_yeoJohnson = []\n",
    "accuracy_quantUniform = []\n",
    "accuracy_quantGaussian = []\n",
    "accuracy_normalizer = []\n",
    "\n",
    "'''\n",
    "#Sense normalització\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_sense.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nSense Scaler:')\n",
    "print('\\nMaximum Accuracy that can be obtained from this model is:',\n",
    "      max(accuracy_sense)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_sense)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_sense)*100, '%')\n",
    "    \n",
    "\n",
    "#MinMax\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    x_train = minmax.fit_transform(x_train)\n",
    "    x_test = minmax.transform(x_test)\n",
    "    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_minmax.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nMinMax Scaler')\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "      max(accuracy_minmax)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_minmax)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_minmax)*100, '%')\n",
    "\n",
    "\n",
    "#Standard_Scaler\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    x_train = standard.fit_transform(x_train)\n",
    "    x_test = standard.transform(x_test)\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_standard.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nStandard Scaler:')\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "      max(accuracy_standard)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_standard)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_standard)*100, '%')\n",
    "\n",
    "#Robust_Scaler\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    \n",
    "    x_train = robust.fit_transform(x_train)\n",
    "    x_test = robust.transform(x_test)\n",
    "    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_robust.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nRobust Scaler:')\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "      max(accuracy_robust)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_robust)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_robust)*100, '%')\n",
    "'''\n",
    "\n",
    "#Yeo-Johson\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):   \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    \n",
    "    x_train = yeoJohnson.fit_transform(x_train)\n",
    "    x_test = yeoJohnson.transform(x_test)\n",
    "    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_yeoJohnson.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nYeo-Johnson Scaler:')\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "      max(accuracy_yeoJohnson)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_yeoJohnson)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_yeoJohnson)*100, '%')\n",
    "\n",
    "'''\n",
    "#Quantile Transformer (Uniform)\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):   \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    \n",
    "    x_train = quantileUniform.fit_transform(x_train)\n",
    "    x_test = quantileUniform.transform(x_test)\n",
    "    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_quantUniform.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nQuantile Transformer (Uniform):')\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "      max(accuracy_quantUniform)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_quantUniform)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_quantUniform)*100, '%')\n",
    "\n",
    "\n",
    "#Quantile Transformer (Gaussian)\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):   \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    \n",
    "    x_train = quantileGaussian.fit_transform(x_train)\n",
    "    x_test = quantileGaussian.transform(x_test)\n",
    "    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_quantGaussian.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nQuantile Transformer (Gaussian):')\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "      max(accuracy_quantGaussian)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_quantGaussian)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_quantGaussian)*100, '%')\n",
    "\n",
    "\n",
    "\n",
    "#Normalizer\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):   \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    \n",
    "    x_train = normalizer.fit_transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logReg.fit(x_train, y_train)\n",
    "    accuracy_normalizer.append(logReg.score(x_test, y_test))\n",
    "\n",
    "print('\\nNormalizer:')\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "      max(accuracy_normalizer)*100, '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "      min(accuracy_normalizer)*100, '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "      mean(accuracy_normalizer)*100, '%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La que ha donat millor resultat ha sigut la normalització Yeo-Johnson, per lo tant comentaré totes les altres perque s'executi més rapid, i mostraré els resultats obtinguts amb els altres tipus de normalització a la memória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca = PCA(n_components=dataset.shape[1])\n",
    "pca.fit(dataset)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El que podem observar en els resultats del PCA es que el primer Principal Component és basicament l'unic que importa ja que te moltíssima més variancia que el segon i els altres, i que l'atribut que aporta la majoria de variancia a aquest primer principal component és el movie_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "selector = RFE(logReg, n_features_to_select=10)\n",
    "selector = selector.fit(x, y)\n",
    "selector.support_\n",
    "x_train = selector.transform(x_train)\n",
    "x_test = selector.transform(x_test)\n",
    "logReg.fit(x_train, y_train)\n",
    "print(logReg.score(x_test, y_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He provat de fer feature selection amb el wrapper RFE, pero el resultat ha donat pitjor del que teniem abans (per lo tant ho comentarem ja que tarda 12 minuts en acabar). També provaré intrinsic feature selection, pero ho faré en la model selection provant el random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hem provat diferents algoritmes d'outlier removal amb l'esperança de millorar l'accuracy i de reduïr la mida de les dades. Aquests algoritmes son: Isolation Forest, OneClassSvm, OneClassSvm(polynomial), LocalOutlierFactor, LocalOutlierFactor(BallTree) i Elliptic Envelope. (Com han tardat molt en executar-se i tots han donat una accuracy pitjor a la que teniem amb outliers, els deixem comentats, i mostrem els resultats a la memòria). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer observem les dades amb histogrames ara que ja estan normalitzades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(x_train, columns = ['movie_id','openness','agreeableness','emotional_stability','conscentiousness','extraversion','all','diversity','popularity','serendipity','high','low','medium'])\n",
    "df2.hist(figsize=[12, 12], ylabelsize=8, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "isoForest = IsolationForest()\n",
    "svm = OneClassSVM()\n",
    "svmPoly = OneClassSVM(kernel=\"poly\")\n",
    "localOutlier = LocalOutlierFactor()\n",
    "localBallTree = LocalOutlierFactor(algorithm=\"ball_tree\")\n",
    "ellipEnvelope = EllipticEnvelope()\n",
    "'''\n",
    "outliersIso = isoForest.fit_predict(x_train)\n",
    "outliersSvm = svm.fit_predict(x_train)\n",
    "outliersSvmPoly = svmPoly.fit_predict(x_train)\n",
    "outliersLocal = localOutlier.fit_predict(x_train)\n",
    "outliersBallTree = localBallTree.fit_predict(x_train)\n",
    "outliersElliptic = ellipEnvelope.fit_predict(x_train)\n",
    "\n",
    "\n",
    "print(f\"Shape amb outliers: {x_train.shape}\")\n",
    "x_trainIso = x_train[outliersIso == 1]\n",
    "y_trainIso = y_train[outliersIso == 1]\n",
    "print(f\"Shape després de IsolationForest: {x_trainIso.shape}\")\n",
    "x_trainSvm = x_train[outliersSvm == 1]\n",
    "y_trainSvm = y_train[outliersSvm == 1]\n",
    "print(f\"Shape després de OneClassSVM(rbf): {x_trainSvm.shape}\")\n",
    "x_trainSvmPoly = x_train[outliersSvmPoly == 1]\n",
    "y_trainSvmPoly = y_train[outliersSvmPoly == 1]\n",
    "print(f\"Shape després de OneClassSVM(poly): {x_trainSvmPoly.shape}\")\n",
    "x_trainLocal = x_train[outliersLocal == 1]\n",
    "y_trainLocal = y_train[outliersLocal == 1]\n",
    "print(f\"Shape després de LocalOutlierFactor(auto): {x_trainLocal.shape}\")\n",
    "x_trainBallTree = x_train[outliersBallTree == 1]\n",
    "y_trainBallTree = y_train[outliersBallTree == 1]\n",
    "print(f\"Shape després de LocalOutlierFactor(ball_tree): {x_trainBallTree.shape}\")\n",
    "x_trainElliptic = x_train[outliersElliptic == 1]\n",
    "y_trainElliptic = y_train[outliersElliptic == 1]\n",
    "print(f\"Shape després de EllipticEnvelope: {x_trainElliptic.shape}\")\n",
    "\n",
    "x_train= x_trainElliptic\n",
    "y_trian = y_trainElliptic\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "logReg.fit(x_trainIso, y_trainIso)\n",
    "print(f\"Accuracy amb IsolationForest: {logReg.score(x_test, y_test)}\")\n",
    "\n",
    "logReg.fit(x_trainSvm, y_trainSvm)\n",
    "print(f\"Accuracy amb OneClassSVM(rbf): {logReg.score(x_test, y_test)}\")\n",
    "\n",
    "logReg.fit(x_trainSvmPoly, y_trainSvmPoly)\n",
    "print(f\"Accuracy amb OneClassSVM(poly): {logReg.score(x_test, y_test)}\")\n",
    "\n",
    "logReg.fit(x_trainLocal, y_trainLocal)\n",
    "print(f\"Accuracy amb LocalOutlierFactor(auto): {logReg.score(x_test, y_test)}\")\n",
    "\n",
    "logReg.fit(x_trainBallTree, y_trainBallTree)\n",
    "print(f\"Accuracy amb LocalOutlierFactor(ball_tree): {logReg.score(x_test, y_test)}\")\n",
    "\n",
    "logReg.fit(x_trainElliptic, y_trainElliptic)\n",
    "print(f\"Accuracy amb EllipticEnvelope: {logReg.score(x_test, y_test)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(f\"Shape amb outliers: {x_train.shape}\")\n",
    "x_trainIso = x_train[outliersIso == 1]\n",
    "y_trainIso = y_train[outliersIso == 1]\n",
    "print(f\"Shape després de IsolationForest: {x_trainIso.shape}\")\n",
    "x_trainSvm = x_train[outliersSvm == 1]\n",
    "y_trainSvm = y_train[outliersSvm == 1]\n",
    "print(f\"Shape després de OneClassSVM(rbf): {x_trainSvm.shape}\")\n",
    "x_trainSvmPoly = x_train[outliersSvmPoly == 1]\n",
    "y_trainSvmPoly = y_train[outliersSvmPoly == 1]\n",
    "print(f\"Shape després de OneClassSVM(poly): {x_trainSvmPoly.shape}\")\n",
    "x_trainLocal = x_train[outliersLocal == 1]\n",
    "y_trainLocal = y_train[outliersLocal == 1]\n",
    "print(f\"Shape després de LocalOutlierFactor(auto): {x_trainLocal.shape}\")\n",
    "x_trainBallTree = x_train[outliersBallTree == 1]\n",
    "y_trainBallTree = y_train[outliersBallTree == 1]\n",
    "print(f\"Shape després de LocalOutlierFactor(ball_tree): {x_trainBallTree.shape}\")\n",
    "x_trainElliptic = x_train[outliersElliptic == 1]\n",
    "y_trainElliptic = y_train[outliersElliptic == 1]\n",
    "print(f\"Shape després de EllipticEnvelope: {x_trainElliptic.shape}\")\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com dona millor accuracy sense Outlier Removal, no aplicarem ningún dels mètodes provats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He provat els models classificadors: Logistic Regression, Svc, Linear Svc, Svc(rbf), Svc(Polynomial), KNeighbors Classifier, i Perceptron. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit comentari: Els models que he deixat comentats son els que tarden uns dies per si ho tens que executar. En la memória ja enseño els resultats + el temps que han tardat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "svc = SVC(kernel=\"linear\") \n",
    "linearsvc = LinearSVC(max_iter=100000)\n",
    "svcRBF = SVC(kernel=\"rbf\")\n",
    "svcPoly = SVC(kernel=\"poly\")\n",
    "nn = KNeighborsClassifier()\n",
    "percep = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    x_train = yeoJohnson.fit_transform(x_train)\n",
    "    x_test = yeoJohnson.transform(x_test)\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    logReg.fit(x_train, y_train)\n",
    "    preds = logReg.predict(x_test)\n",
    "    \n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    f1s.append(f1_score(y_test, preds, average=\"micro\"))\n",
    "    precs.append(precision_score(y_test, preds, average=\"micro\"))\n",
    "    recs.append(recall_score(y_test, preds, average=\"micro\"))\n",
    "    \n",
    "acc = mean(accs)\n",
    "f1 = mean(f1s)\n",
    "prec = mean(precs)\n",
    "rec = mean(recs)\n",
    "    \n",
    "print(f\"Logistic Regression: Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "accs = []\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    x_train = yeoJohnson.fit_transform(x_train)\n",
    "    x_test = yeoJohnson.transform(x_test)\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    svc.fit(x_train, y_train)\n",
    "    preds = svc.predict(x_test)\n",
    "\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    f1s.append(f1_score(y_test, preds, average=\"micro\"))\n",
    "    precs.append(precision_score(y_test, preds, average=\"micro\"))\n",
    "    recs.append(recall_score(y_test, preds, average=\"micro\"))\n",
    "    \n",
    "acc = mean(accs)\n",
    "f1 = mean(f1s)\n",
    "prec = mean(precs)\n",
    "rec = mean(recs)\n",
    "\n",
    "print(f\"SVC: Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "print(\"\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    x_train = yeoJohnson.fit_transform(x_train)\n",
    "    x_test = yeoJohnson.transform(x_test)\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    linearsvc.fit(x_train, y_train)\n",
    "    preds = linearsvc.predict(x_test)\n",
    "\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    f1s.append(f1_score(y_test, preds, average=\"micro\"))\n",
    "    precs.append(precision_score(y_test, preds, average=\"micro\"))\n",
    "    recs.append(recall_score(y_test, preds, average=\"micro\"))\n",
    "    \n",
    "acc = mean(accs)\n",
    "f1 = mean(f1s)\n",
    "prec = mean(precs)\n",
    "rec = mean(recs)\n",
    "    \n",
    "print(f\"linearSVC: Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "accs = []\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    x_train = yeoJohnson.fit_transform(x_train)\n",
    "    x_test = yeoJohnson.transform(x_test)\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    svcRBF.fit(x_train, y_train)\n",
    "    preds = svcRBF.predict(x_test)\n",
    "\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    f1s.append(f1_score(y_test, preds, average=\"micro\"))\n",
    "    precs.append(precision_score(y_test, preds, average=\"micro\"))\n",
    "    recs.append(recall_score(y_test, preds, average=\"micro\"))\n",
    "    \n",
    "acc = mean(accs)\n",
    "f1 = mean(f1s)\n",
    "prec = mean(precs)\n",
    "rec = mean(recs)\n",
    "    \n",
    "print(f\"SVC(rbf): Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "print(\"\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "accs = []\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    x_train = yeoJohnson.fit_transform(x_train)\n",
    "    x_test = yeoJohnson.transform(x_test)\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    svcPoly.fit(x_train, y_train)\n",
    "    preds = svcPoly.predict(x_test)\n",
    "\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    f1s.append(f1_score(y_test, preds, average=\"micro\"))\n",
    "    precs.append(precision_score(y_test, preds, average=\"micro\"))\n",
    "    recs.append(recall_score(y_test, preds, average=\"micro\"))\n",
    "    \n",
    "acc = mean(accs)\n",
    "f1 = mean(f1s)\n",
    "prec = mean(precs)\n",
    "rec = mean(recs)\n",
    "    \n",
    "print(f\"SVC(Polynomial): Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "print(\"\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knns = dict()\n",
    "for k in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]:\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "\n",
    "    nn = KNeighborsClassifier(n_neighbors=k)\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        x_train = yeoJohnson.fit_transform(x_train)\n",
    "        x_test = yeoJohnson.transform(x_test)\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        nn.fit(x_train, y_train)\n",
    "        preds = nn.predict(x_test)\n",
    "        \n",
    "        accs.append(accuracy_score(y_test, preds))\n",
    "        f1s.append(f1_score(y_test, preds, average=\"micro\"))\n",
    "        precs.append(precision_score(y_test, preds, average=\"micro\"))\n",
    "        recs.append(recall_score(y_test, preds, average=\"micro\"))\n",
    "    \n",
    "    acc = mean(accs)\n",
    "    f1 = mean(f1s)\n",
    "    prec = mean(precs)\n",
    "    rec = mean(recs)\n",
    "    print(f\"Nearest Neighbors(k = {k}): Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "    print(\"\")\n",
    "    \n",
    "    knns[k] = nn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    x_train = yeoJohnson.fit_transform(x_train)\n",
    "    x_test = yeoJohnson.transform(x_test)\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    percep.fit(x_train, y_train)\n",
    "    preds = percep.predict(x_test)\n",
    "\n",
    "    accs.append(accuracy_score(y_test, preds))\n",
    "    f1s.append(f1_score(y_test, preds, average=\"micro\"))\n",
    "    precs.append(precision_score(y_test, preds, average=\"micro\"))\n",
    "    recs.append(recall_score(y_test, preds, average=\"micro\"))\n",
    "    \n",
    "acc = mean(accs)\n",
    "f1 = mean(f1s)\n",
    "prec = mean(precs)\n",
    "rec = mean(recs)\n",
    "    \n",
    "print(f\"Perceptron: Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sense k-fold perque va extremadament lent\n",
    "rfcs = dict()\n",
    "for t in range(10, 201, 20):\n",
    "    rfc = RandomForestClassifier(n_estimators=t)\n",
    "    rfc.fit(x_train, y_train)\n",
    "    preds = rfc.predict(x_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    prec = precision_score(y_test, preds, average=\"micro\")\n",
    "    rec = recall_score(y_test, preds, average=\"micro\")\n",
    "    \n",
    "    print(f\"Random Forest(trees = {t}): Accuracy = {acc:.3f}; f1_score = {f1:.3f}; Precision = {prec:.3f}; Recall = {rec:.3f};\")\n",
    "    print(\"\")\n",
    "    \n",
    "    rfcs[t] = rfc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e49f01b25252117987057767e4860399151593b17ce2349b8fe42cec150c223d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
